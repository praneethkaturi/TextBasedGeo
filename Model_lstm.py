# -*- coding: utf-8 -*-
"""Model_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MUl_X9p0AQVQbY3GvTnJCMpZ1IqwsCS6

# Importing the basic libraries and setting dataset
"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd
import numpy as np

States = pd.read_csv('/content/gdrive/My Drive/Research Lab/GeoTagging/States_LSTM.csv', sep= ',', header= None)

tweets = pd.read_csv('/content/gdrive/My Drive/Research Lab/GeoTagging/final_Tweets (1).csv', header= None)

States.drop(labels= 0, axis= 1, inplace= True)
tweets.dropna(axis= 0, inplace= True)
number = np.random.randint(low = 0, high= 376509, size= 1, dtype = 'int')
tweets.drop(labels= number[0], axis= 0, inplace = True)
tweets.drop(labels= 0, axis = 1, inplace= True)
States.columns = ['State']

tweets.index = range(tweets.shape[0])



remove = ['Ontario','Quebec','Wyoming', 'Baja California Sur', 'Nacional', 'Kerala', 'Sonora','Chelyabinsk', 
         'Scotland', 'Pays de la Loire', 'Coahuila', 'Saskatchewan', 'Hesse', 'Chihuahua', 'Xinjiang Uygur Zizhiqu',
         'Hamilton city', 'Punjab', 'Maine', 'Tamaulipas', 'Taipei', 'Quindio', 'Portland', 'Karnataka', 'Baja California'
         'Maharashtra', 'Ar Riyad', 'Mayaguana', 'Moskovskaya', 'Gauteng', 'Sinaloa', 'Jakarta Raya', 'Rio de Janeiro', 'Bangkok', 'Bali'
         'North Sulawesi', 'Vega Alta', 'Quintana Roo', 'Saint George', 'Prince Edward Island', 'Maharashtra', 'Bali', 'Santo Domingo ',
            'North Sulawesi', 'New Providence', 'Colorado', 'Nuevo Leon', 'Kansas', 'Iowa', 'West Virginia', 'New Mexico', 'Utah',
            'Nebraska', 'Idaho', 'Nova Scotia','British Columbia', 'Baja California', 'Vermont', 'North Dakota', 'New Hampshire',
            'South Dakota', 'New Brunswick','West Java', 'East Java', 'Montana', 'Santo Domingo']

index = []
for element in remove:
    io = States[States.State == element].index
    index.append(io)

for element in index:
    States.drop(labels= element, axis= 0, inplace= True)
    tweets.drop(labels= element, axis= 0, inplace= True)

States.index = range(States.shape[0])
tweets.index = range(tweets.shape[0])

assert(tweets.shape[0] == States.shape[0])

states = States.State.unique()

states = [i for i in states]

northeast = ['Connecticut', 'Maine','Massachusetts', 'New Hampshire', 'Rhode Island', 'Vermont',
            'New Jersey', 'New York', 'Pennsylvania']

midwest = ['Illinois', 'Indiana', 'Michigan', 'Ohio','Wisconsin','Iowa', 'Kansas', 'Minnesota',
           'Missouri', 'Nebraska', 'North Dakota', 'South Dakota']

south = ['Delaware', 'Florida', 'Georgia', 'Maryland', 'North Carolina', 'South Carolina', 'Virginia',
         'Washington, D.C.', 'West Virginia', 'Alabama', 'Kentucky', 'Mississippi', 'Tennessee',
        'Arkansas', 'Louisiana', 'Oklahoma', 'Texas']

west = ['Arizona', 'Colorado', 'Idaho', 'Montana', 'Nevada', 'New Mexico', 'Utah', 'Wyoming',
       'Alaska', 'California', 'Hawaii', 'Oregon', 'Washington']

region = []
for state in states:
  if state in northeast:
    region.append('NorthEast')
    pass
  
  if state in midwest:
    region.append('Midwest')
    pass
  
  if state in south:
    region.append('South')
    pass
  
  if state in west:
    region.append('West')
    pass

region_dict = {'State': states, 'Region': region}

region_pd = pd.DataFrame(region_dict)

States = States.join(region_pd.set_index('State'), on= 'State')

Region = pd.DataFrame(States.Region)
Region.head()

"""# Setting the vocabulary"""

tweets_np = np.array(tweets).reshape(tweets.shape[0],)

X = [tweets_np[i].lower().split() for i in range(len(tweets_np))]

from collections import Counter
from itertools import chain,chain

vocab = Counter(chain.from_iterable([X[i] for i in range(len(X))]))

vocab = [i for i in vocab if vocab[i]>2]

vocab.append('#PAD')
vocab.sort()

vocabulary = {token: index for index, token in enumerate(vocab)}

assert(len(vocab) == len(vocabulary))

"""# Padding the tweets and converting sentences to indices"""

#max length of a tweet is restricted to 100. Change max_len for different result
max_len = 20
pad_idx = vocabulary['#PAD']

tweets_np = [[vocabulary[word] if word in vocabulary.keys() else vocabulary['#PAD'] for word in words] for words in X]

tweets_np = np.array(tweets_np)

tweets_np.shape

tweets_matrix = np.zeros((len(tweets_np), max_len)) + pad_idx
for i in range(len(tweets_np)):
  column = min(max_len-1, len(tweets_np[i])-1)
  if(len(tweets_np[i]) != 0):
    tweets_matrix[i, 0:column] = tweets_np[i][0:column]

tweets_matrix.shape

"""# Converting States to one-hot"""

States_onehot = pd.get_dummies(Region)

Y = np.array(States_onehot)

Y.shape

"""# Model"""

import tensorflow as tf
import keras
from keras.models import Model
from keras.layers import Dense, Input, Dropout, LSTM, Activation
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
from keras.initializers import glorot_uniform

from sklearn.model_selection import train_test_split

vocab_length = len(vocabulary) + 1
emb_dim = 50

embedding_layer = Embedding(input_dim= vocab_length, output_dim= emb_dim, input_length= max_len, trainable = True)

embedding_layer.build(input_shape= (None,))

def Emojify_V2(input_shape, vocabulary):
    """
    Function creating the Emojify-v2 model's graph.
    
    Arguments:
    input_shape -- shape of the input, usually (max_len,)
    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation
    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)

    Returns:
    model -- a model instance in Keras
    """
    
    ### START CODE HERE ###
    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).
    sentence_indices = Input(shape= input_shape, dtype= 'int32')
    
    # Create the embedding layer pretrained with GloVe Vectors (â‰ˆ1 line)
    
    # Propagate sentence_indices through your embedding layer, you get back the embeddings
    embeddings = embedding_layer(sentence_indices)
    
    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state
    # Be careful, the returned output should be a batch of sequences.
    X = keras.layers.GRU(units= 128, kernel_initializer= 'glorot_uniform', activation= 'relu', return_sequences= True)(embeddings)
    # Add dropout with a probability of 0.5
    # Propagate X trough another LSTM layer with 128-dimensional hidden state
    # Be careful, the returned output should be a single hidden state, not a batch of sequences.
    X = keras.layers.GRU(units= 256, activation= 'relu', return_sequences= True)(X)
    X = keras.layers.Dropout(rate = 0.5)(X)
    X = keras.layers.BatchNormalization()(X)
    X = keras.layers.GRU(units= 256, activation= 'relu', return_sequences= True)(X)
    X = keras.layers.Dropout(rate = 0.5)(X)
    X = keras.layers.BatchNormalization()(X)
    X = keras.layers.GRU(units= 256, activation= 'relu', return_sequences= False)(X)
    # Add dropout with a probability of 0.5
    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.
    X = Dense(units= 4, activation= 'softmax')(X)
    # Add a softmax activation
    X = Activation(activation= 'softmax')(X)
    
    # Create Model instance which converts sentence_indices into X.
    model = Model(inputs= sentence_indices, outputs= X)
    
    ### END CODE HERE ###
    
    return model

model = Emojify_V2((max_len,), vocabulary)
model.summary()

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

X_train, X_rem, Y_train, Y_rem = train_test_split(tweets_matrix, Y, test_size = 0.1)

X_val, X_test, Y_val, Y_test = train_test_split(X_rem, Y_rem, test_size = 0.1)

model.fit(X_train, Y_train, epochs = 60, batch_size = 1024, shuffle=True)

model.evaluate(x= X_val, y= Y_val, verbose= 1, batch_size= 64)

model.evaluate(x= X_test, y= Y_test, verbose= 1, batch_size= 64)

