# -*- coding: utf-8 -*-
"""Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NTom7FmZlU3dHAbH8ScfR5bc6uIBWKOf
"""

import tensorflow as tf
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

from keras.utils import to_categorical

Y_classes = pd.read_csv('States_final.csv', sep= ',', header= None)
X_tweets = pd.read_csv('Tweets_final.csv', sep= ',', header= None)

Y_classes.head()

X_tweets.head()

Y_classes[pd.isnull(X_tweets).any(axis= 1)]

Y_classes.drop(labels= [0, 1], inplace= True, axis= 1)
X_tweets.drop(labels=[0], inplace= True, axis= 1)

Y_classes[2].unique()

remove = ['Ontario','Quebec','Wyoming', 'Baja California Sur', 'Nacional', 'Kerala', 'Sonora','Chelyabinsk', 
         'Scotland', 'Pays de la Loire', 'Coahuila', 'Saskatchewan', 'Hesse', 'Chihuahua', 'Xinjiang Uygur Zizhiqu',
         'Hamilton city', 'Punjab', 'Maine', 'Tamaulipas', 'Taipei', 'Quindio', 'Portland', 'Karnataka', 'Baja California'
         'Maharashtra', 'Ar Riyad', 'Mayaguana', 'Moskovskaya', 'Gauteng', 'Sinaloa', 'Jakarta Raya', 'Rio de Janeiro', 'Bangkok', 'Bali'
         'North Sulawesi', 'Vega Alta', 'Quintana Roo', 'Saint George', 'Prince Edward Island']

indexes = []
for element in remove:
    index = Y_classes[Y_classes[2] == element].index
    indexes.append(index)

for new in indexes:
    Y_classes.drop(labels= new, inplace= True, axis= 0)

for new in indexes:
    X_tweets.drop(labels= new, inplace= True, axis= 0)

Y_oh = pd.get_dummies(Y_classes)

X = np.array(X_tweets)

Y = np.array(Y_oh)

print(Y.shape)
print(X.shape)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size= 0.1)

from keras.models import Sequential
import keras.layers as layers

import keras

def make_model():
    model = Sequential()
    model.add(layers.Dense(input_shape= (50, ) ,units= 64 , activation= tf.nn.sigmoid, kernel_initializer= tf.initializers.truncated_normal()))
    model.add(layers.Dense(units= 64 , activation= tf.nn.sigmoid, kernel_initializer= tf.initializers.truncated_normal()))
    model.add(layers.Dense(units=64 , activation= tf.nn.tanh, kernel_initializer= tf.initializers.truncated_normal()))
    model.add(layers.Dense(units= 59, activation=tf.nn.leaky_relu, kernel_initializer= 'glorot_uniform'))
    model.add(layers.Activation('softmax'))
    return model

tf.reset_default_graph()

# K.clear_session()  # clear default graph
model = make_model()
model.summary()

model.compile( loss='categorical_crossentropy',  
    optimizer= tf.train.AdamOptimizer(0.01),  
    metrics=['accuracy']
)

Epochs = 50
batch_size = 10000

model.fit(x = X_train, y= Y_train, batch_size= batch_size, epochs= Epochs, verbose= 1, shuffle= True)

